[{"categories":["basic"],"contents":"Context Consistently launching updates on the App Store and Play Store can be more complicated than it seems, particularly managing multiple apps at the same time. Exploring the workflows of different teams is fascinating. By identifying commonalities and distinctions, we can uncover potentially beneficial strategies. Here is an overview of eyLog\u0026rsquo;s mobile release management.\nOverview of Release cycle Our release cycles are pretty similar to what many other companies do. Here’s a quick look at how we handle things:\nWe release a new version of the app every month, so we have a release cut every month Testing starts as soon as we cut a release. We have about a week to fix any major issues, and we do this by adding specific fixes to the release branch After testing and fixing are done, we submit the app for App Store review, usually in the middle of the month. This timing gives us some extra time in case there are any rejections or delays in the review process. Once we get approval, the build is kept ready for release until the end of the month. On release day, we start by rolling out the update by phased release for Automatic Updates to small perecentage of our users. This helps us catch any big problems that we might have missed during testing. After carefully watching how the release is performing by usinng new reliac , we speed up the rollout to all users by Day 7 While this is happening, we’re already working on the next release, so it’s a continuous cycle. Managing Communication To keep communication clear and organized during releases and rollouts, we create a Cliq channel for each release, like #ios-release-6-1-0. This helps centralize updates and discussions about a specific release, which reduces distractions in other channels and makes it easy to find information about past releases if needed.\nHaving a dedicated Cliq channel is especially useful when we need to issue hotfixes. Since we release updates weekly, a hotfix for an existing issue means that two releases are happening at the same time. Keeping each release’s communication in its own channel helps avoid confusion. For example, anything related to a hotfix for version 6.1.0 would be discussed in #ios-release-6-1-0, while discussions about the upcoming version 6.2.0 would take place in #ios-release-6-2-0.\nTesting Release Candidates Our apps are so big that it\u0026rsquo;s not possible for just one person or even a small group to handle all the testing for release candidates. The team that focuses on quality assurance (QA) can\u0026rsquo;t keep up with the heavy weekly testing needed either. With so many changes and new features being developed quickly, it’s hard to make sure the right things are being tested correctly. The people who are actually building the features know best what’s new, what’s changed, and how to test it.\nThat’s why we rely on a group of engineers called \u0026ldquo;watchdog\u0026rdquo; consist of backend , frontend \u0026amp; mobile engineers for testing release candidates. Each watchdog is in charge of testing their own part of the app, whether it\u0026rsquo;s a feature or section, and they fix any issues found during testing or delegate fixes to others. Each part generally matches up with our product teams—for example, the login team tests the login feature of the app. Each watchdog has a specific set of tests they must run before they can approve their component. Every component has to be approved before we can submit the release for review.\nHandling Issues During Release Candidate Testing If a watchdog finds a problem during release candidate testing, they work with their team to figure out the issue and then create a fix on our main branch using a standard pull request. Once the fix is merged, it might be eligible to be added to the release branch through a process called cherry-picking. However, because adding changes at the last minute can be risky, we have strict rules about what can be included in a release.\nWe allow fixes for serious problems or new bugs that affect users\u0026rsquo; experience, but we don’t allow minor bug fixes that don\u0026rsquo;t impact users and we definitely won\u0026rsquo;t use this process to add new features that missed the deadline. We developed a system for reviewing fix requests, where teams submit requests that explain the issue and provide evidence, which are then checked by the lead.\nAfter the release, the process for making a hotfix is similar to requesting a cherry-pick during a release cycle, but the rules are even stricter. Creating a hotfix requires more effort and could interfere with upcoming normal releases. If we discover a bug late in the release cycle—like after the app has been submitted for review but before it goes live—the decision to fix it will depend on the same strict rules as for post-release hotfixes.\nEven if the update isn\u0026rsquo;t public yet, it could be waiting for review or already approved. To fix it, we’d need to reject the current build and resubmit the app. Since this could delay the release, we evaluate whether a fix is necessary and how to handle it on a case-by-case basis. We might choose to reject the build, apply a fix, and then resubmit, or allow the scheduled release to happen and issue a hotfix afterward. Alternatively, we might decide that the bug isn’t serious enough to need a hotfix, waiting to fix it in the next release cycle.\nMonitoring rollouts post-release Lead and respective developer both are responisble for monitoring post release. We use New Reliac and Firebase to trackdown crashes and health. If lead sees something unusual, he delegates respective developer to take a deeper look and make fixes as needed. But if no problems arise, we can automatically go to a full rollout.\n","date":"January 20, 2025","hero":"/images/site/eyw.png","permalink":"https://chowdhurynawaf.github.io/en/posts/how-eylog-manages-mobile-releases/","summary":"\u003ch3 id=\"context\"\u003e\u003cstrong\u003eContext\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eConsistently launching updates on the App Store and Play Store can be more complicated than it seems, particularly managing multiple apps at the same time. Exploring the workflows of different teams is fascinating. By identifying commonalities and distinctions, we can uncover potentially beneficial strategies. Here is an overview of eyLog\u0026rsquo;s mobile release management.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"overview-of-release-cycle\"\u003e\u003cstrong\u003eOverview of Release cycle\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eOur release cycles are pretty similar to what many other companies do. Here’s a quick look at how we handle things:\u003c/p\u003e","tags":["eyLog","Release"],"title":"How eyLog Manages Mobile Releases"},{"categories":["basic"],"contents":"To deliver a fast and seamless user experience, every application requires an effective caching system. Without caching, repeatedly downloading assets can consume significant resources and cause delays in loading times. To overcome this, developers use a variety of caching strategies, including disk caching, LRU (Least Recently Used) caching, database caching, and others, to enhance performance and minimize latency.\nProblem: In the Beeda User app, effectively handling media assets like images, Lottie animations, and videos was essential for ensuring a fast and seamless user experience. Each type of media asset comes with its own specific needs and challenges when it comes to caching and optimization:\nImages: High-resolution images can consume significant memory and storage, making disk caching (e.g., using URLCache or third-party libraries like SDWebImage) essential. Lottie Animations: Lottie files (JSON-based animations) are lightweight but require efficient parsing and rendering. In-memory caching (e.g., using NSCache) can help reduce repeated parsing overhead. Videos: Videos are large in size and require streaming or partial caching to avoid excessive memory and storage usage. While each caching mechanism is effective for specific media types, a critical question arises: How can we effectively manage and integrate multiple caching strategies for images, Lottie animations, and videos within an iOS app?\nImplementation: To address the challenges of managing media assets in the Beeda User app, we’ve designed different Utils for differnt type accordingly. This structured approach ensures efficient handling of assets like images, Lottie animations, and videos.\n1. Utils Layer Each type of media asset will have its own utility class responsible for handling specific tasks such as downloading, parsing, and processing. These utilities act as intermediaries, fetching assets from the Assets Manager without the app directly interacting with it.\nFor example, consider the Image Utility. If an image is not already cached, this utility will download it by calling the getData method of the Assets Manager. Once the image is downloaded, the utility will pass the raw data back to the Assets Manager for storage.\nprotocol ImageUtilProtocol { func downloadImage(url: String, imageCompletionHandler: ((UIImage) -\u0026gt; Void)?, storageType: StorageType) } 2. Assets Manager The Assets Manager serves as the middle layer in the system, acting as a bridge between the Utils and the various caching mechanisms. Its primary role is to determine which type of caching should be used based on the specified storage type.\nThe StorageType parameter will define the available caching mechanisms (e.g., in-memory, disk, or hybrid).\nenum StorageType { case lru(LRUCacheType) case disk } enum LRUCacheType: String { case lottie case image case video } The assets manager will have common methods to store and retrieve data.\nprotocol AssetsManagerProtocol { func writeData(data: Data, forKey key: String, storageType: StorageType) func readData(forKey key: String, storageType: StorageType) -\u0026gt; Data? func removeData(forKey key: String, storageType: StorageType) } Assets Manager plays a crucial role in managing caching operations. When the utility classes call its methods, the Assets Manager determines the appropriate caching layer to use based on the storageType. It then delegates the task—whether writing, reading, or removing data—to the corresponding caching mechanism, ensuring efficient and accurate handling of assets.\n3. Caching Layer This layer contains all the caching methods we use, which includes disk, LRU, database caching. It stores data in its raw form and remains independent of specific data types.\nWe employ various caching mechanisms depending on the use case. For assets that need to be stored permanently, we rely on disk caching. On the other hand, for temporary data, we utilize the LRU caching method. Additionally, within the LRU caching system, we have separate caches for different types of assets, such as images, Lottie animations, and others.\nDisk Caching Disk caching involves storing data directly on the disk, enabling efficient read and write operations as required.\nLRU Caching Least Recently Used (LRU) caching stores data in memory with a fixed size limit. When the memory reaches its capacity, the least recently accessed item is evicted to accommodate new data.\nIn Our Case: For Images: We utilize both disk caching and LRU caching to balance performance and storage. For Rendering Animations: We rely exclusively on LRU caching for faster access and efficient memory usage. For Rendering Videos: We employ a custom disk cache tailored for handling large video files. Each caching layer operates independently and is responsible for its designated task. If we need to introduce a new caching mechanism in the future, it can be seamlessly integrated without disrupting.\n","date":"January 20, 2024","hero":"/images/site/b1.jpg","permalink":"https://chowdhurynawaf.github.io/en/posts/cache-management-strategy-in-beeda/","summary":"\u003cp\u003eTo deliver a fast and seamless user experience, every application requires an effective caching system. Without caching, repeatedly downloading assets can consume significant resources and cause delays in loading times. To overcome this, developers use a variety of caching strategies, including \u003cstrong\u003edisk caching\u003c/strong\u003e, \u003cstrong\u003eLRU (Least Recently Used) caching\u003c/strong\u003e, \u003cstrong\u003edatabase caching\u003c/strong\u003e, and others, to enhance performance and minimize latency.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"problem\"\u003eProblem:\u003c/h3\u003e\n\u003cp\u003eIn the \u003cstrong\u003eBeeda User\u003c/strong\u003e app, effectively handling media assets like images, Lottie animations, and videos was essential for ensuring a fast and seamless user experience. Each type of media asset comes with its own specific needs and challenges when it comes to caching and optimization:\u003c/p\u003e","tags":["Beeda","cache"],"title":"Optimizing Cache Management Strategy in Beeda"},{"categories":["basic"],"contents":"Context As Beeda expanded with new features, compile times began to pose a serious challenge. A simple change or full compilation took around 11 minutes, slowing down our workflow significantly.\nTo enhance productivity and streamline development, we aimed to drastically reduce this compile time. In this article, I’ll walk you through how we successfully brought it down to just 2 minutes.\nOptimization Levels in Xcode Xcode provides three optimization levels to choose from:\nNone Fast Fast with Whole Module Optimization Here’s how we leveraged these settings to achieve this improvement.\nEnabling Whole Module Optimization greatly accelerates the compilation process. However, selecting the Fast or Fast with Whole Module Optimization settings disables debugging functionality. After choosing one of these options and compiling the app, attempting to debug results in the following message in the console:\nApp was compiled with optimization - stepping may behave oddly; variables may not be available.\nSolution: Add a User-Defined Setting To enable full module optimization, you’ll need to manually add a User-Defined Setting in your Xcode project configuration. Here’s how you can do it:\nSteps Navigate to Project Settings:\nIn the Project Navigator, select your project. Go to the Build Settings tab. Add a User-Defined Setting:\nClick the + button in the top-left corner of the Build Settings pane. Select Add User-Defined Setting. Name the setting SWIFT_WHOLE_OPTIMIZATION_LEVEL (or another relevant name). Set its value to YES to enable full module optimization. Set None in Debug configuration\nFrom target build settings set Optimization Level to None in Debug configuration How Does This Impact Beeda? For our iOS team, which frequently updates and iterates on our code, this enhancement plays a significant role in improving efficiency, reducing costs, and enhancing the customer experience. To put it into perspective, if we run 30 compilations each day, this optimization saves us roughly 26 hours of compilation time per day. This time savings is akin to having the output of three extra developers on the team.\n","date":"February 9, 2023","hero":"/images/site/bsup.jpg","permalink":"https://chowdhurynawaf.github.io/en/posts/improving-beedas-swift-compilation-time/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eAs Beeda expanded with new features, compile times began to pose a serious challenge. A simple change or full compilation took around 11 minutes, slowing down our workflow significantly.\u003c/p\u003e\n\u003cp\u003eTo enhance productivity and streamline development, we aimed to drastically reduce this compile time. In this article, I’ll walk you through how we successfully brought it down to just 2 minutes.\u003c/p\u003e\n\u003ch2 id=\"optimization-levels-in-xcode\"\u003eOptimization Levels in Xcode\u003c/h2\u003e\n\u003cp\u003eXcode provides three optimization levels to choose from:\u003c/p\u003e","tags":["Beeda","Optimisation"],"title":"Improving Beeda's Swift Compilation times by 83%"}]